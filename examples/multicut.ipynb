{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multicut\n",
    "\n",
    "Simple example for obtaining the multicut segmentation for the dataset, we have initialized in *init*.\n",
    "\n",
    "Note that for applying the pipeline to 3d data, there need only to be a few changes:\n",
    "\n",
    "* Start from an extended 3d supervoxel oversegmentation instead of flat superpixel.\n",
    "* Set the degree of anisotropy to 1.\n",
    "* Set use_2d to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import vigra\n",
    "\n",
    "# rand index for evaluating the multicut score\n",
    "# note that this doesnt have an ignore label...\n",
    "from sklearn.metrics import adjusted_rand_score as rand_index\n",
    "\n",
    "# imports from pipeline\n",
    "from multicut_src import MetaSet\n",
    "from multicut_src import multicut_workflow\n",
    "from multicut_src import ExperimentSettings\n",
    "from multicut_src import merge_small_segments\n",
    "\n",
    "# get the metaset from the correct cache folder\n",
    "cache_folder = \"/path/to/cache\"\n",
    "#cache_folder = \"/home/consti/Work/data_master/cache_neurocut/cache_examples\"\n",
    "meta = MetaSet(cache_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reload the dataset\n",
    "meta.load()\n",
    "ds = meta.get_dataset(\"dataset\")\n",
    "\n",
    "# id of the segmentation (== superpixels) we will calculate everything for\n",
    "# we have added only one segmentation, so we have to use 0 here\n",
    "seg_id = 0\n",
    "\n",
    "# ExperimentSettings holds all relveant options for the experiments\n",
    "# they are initialised to sensible defaults and \n",
    "# we only have to change a few\n",
    "mc_params = ExperimentSettings()\n",
    "\n",
    "# cache folder for the RF\n",
    "mc_params.set_rfcache(os.path.join(cache_folder, \"rf_cache\"))\n",
    "# train RF with 500 trees\n",
    "mc_params.set_ntrees(500)\n",
    "# degree of anisotropy for the filter calculation\n",
    "# (values bigger than 20 lead to calculation in 2d)\n",
    "# set to 1. for isotropic data (default value)\n",
    "mc_params.set_anisotropy(25.)\n",
    "# flag to indicate whether special z - edge features are computed\n",
    "# set to false for isotropic data (default value)\n",
    "mc_params.set_use2d(True)\n",
    "\n",
    "# otherwise, the default parameter should be ok\n",
    "\n",
    "# list of features taken into account\n",
    "# \"raw\" -> filters on raw data accumulated over the edges\n",
    "# \"prob\" -> filters on probability maps accumulated over the edges\n",
    "# \"reg\" -> region statistics, mapped to the edges\n",
    "# \"topo\" -> topology features for the edges\n",
    "feat_list = (\"raw\", \"prob\", \"reg\", \"topo\")\n",
    "\n",
    "# in multicut_from_rf_gt, first a rf is learned with labels from\n",
    "# groundtruth projections from the training dataset\n",
    "# then this rf is applied to the test dataset and the resulting \n",
    "# probabilites for the edges are converted to energies\n",
    "# with this energies, the multicut is solved\n",
    "\n",
    "# note that we have not done a train / test split yet,\n",
    "# so train and test are the same\n",
    "\n",
    "# first parameter is the dataset we train the rf on\n",
    "# second parameter is the dataset we run the multicut on, with weights from the rf\n",
    "# third and fourth parameter: id of segmentationf for train and test dataset\n",
    "# fifth in six: features for train and test\n",
    "# seventh: the parameter object\n",
    "mc_nodes, mc_edges, mc_energy, t_inf = multicut_workflow(\n",
    "    ds, ds,\n",
    "    seg_id, seg_id,\n",
    "    feat_list, mc_params)\n",
    "# mc_nodes = result for the segments\n",
    "# mc_edges = result for the edges\n",
    "# mc_energy = energy of the solution\n",
    "# t_inf = time the inference of the mc took\n",
    "\n",
    "# project the result back to the volume\n",
    "mc_seg = ds.project_mc_result(seg_id, mc_nodes)\n",
    "# merge small segments\n",
    "mc_seg = merge_small_segments(mc_seg, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate the result\n",
    "gt = ds.gt()\n",
    "\n",
    "# evaluation with proper skneuro randIndex\n",
    "\"\"\"\n",
    "from skneuro.learning import randIndex\n",
    "\n",
    "gt = gt.astype(np.uint32)\n",
    "mc_seg = mc_seg.astype(np.uint32)\n",
    "\n",
    "print randIndex(gt.flatten(), mc_seg.flatten(), ignoreDefaultLabel = True)\n",
    "\"\"\"\n",
    "\n",
    "ri = rand_index(gt.ravel(), mc_seg.ravel())\n",
    "print \"Multicut Segmentation has a RI of\", ri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These numbers don't look too good, but I think this is due to the sklearn metric. Visually, the result does look good and evaluation with the RandIndex from NeuroMetrics yields a value of 0.9998."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}