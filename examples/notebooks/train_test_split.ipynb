{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test - Split\n",
    "\n",
    "Train / test - split of the volume is made, using cutouts. Multicut is run on both subvolumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import vigra\n",
    "import os\n",
    "\n",
    "# rand index for evaluating the multicut score\n",
    "from sklearn.metrics import adjusted_rand_score as rand_index\n",
    "\n",
    "# imports from Neurocut\n",
    "from multicut_src import MetaSet\n",
    "from multicut_src import DataSet\n",
    "from multicut_src import multicut_workflow\n",
    "from multicut_src import ExperimentSettings\n",
    "from multicut_src import merge_small_segments\n",
    "\n",
    "# initialize the MetaSet, that holds all datasets for experiments\n",
    "# folder for saving the cache\n",
    "cache_folder = \"/path/to/cache\"\n",
    "#cache_folder = \"/home/consti/Work/data_master/cache_neurocut/cache_examples\"\n",
    "meta = MetaSet(cache_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make the train / test - split\n",
    "# only call this once !\n",
    "meta.load()\n",
    "ds = meta.get_dataset(\"dataset\")\n",
    "\n",
    "# we train on the lower ten and test on the upper ten\n",
    "shape = ds.shape\n",
    "z_split = int(shape[2] / 2)\n",
    "# for this we use cutouts, which inherit from DataSet, \n",
    "# so we can basically use them in the same manner\n",
    "ds.make_cutout([0,shape[0],0,shape[1],0,z_split])\n",
    "ds.make_cutout([0,shape[0],0,shape[1],z_split,shape[2]])\n",
    "\n",
    "# need to save the changes we have made to the dataset\n",
    "meta.update_dataset(\"dataset\", ds)\n",
    "meta.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we can repeat the experiments from above with the train / test split\n",
    "# reload the dataset\n",
    "meta.load()\n",
    "ds = meta.get_dataset(\"dataset\")\n",
    "ds_train = ds.get_cutout(0)\n",
    "ds_test  = ds.get_cutout(1)\n",
    "seg_id = 0\n",
    "\n",
    "# ExperimentSettings holds all relveant options for the experiments\n",
    "# they are initialised to sensible defaults and \n",
    "# we only have to change a few\n",
    "mc_params = ExperimentSettings()\n",
    "\n",
    "# cache folder for the RF\n",
    "mc_params.set_rfcache(os.path.join(cache_folder, \"rf_cache\"))\n",
    "# train RF with 500 trees\n",
    "mc_params.set_ntrees(500)\n",
    "# degree of anisotropy for the filter calculation\n",
    "# (values bigger than 20 lead to calculation in 2d)\n",
    "# set to 1. for isotropic data (default value)\n",
    "mc_params.set_anisotropy(25.)\n",
    "# flag to indicate whether special z - edge features are computed\n",
    "# set to false for isotropic data (default value)\n",
    "mc_params.set_use2d(True)\n",
    "\n",
    "# otherwise, the default parameter should be ok\n",
    "\n",
    "# list of features taken into account\n",
    "# \"raw\" -> filters on raw data accumulated over the edges\n",
    "# \"prob\" -> filters on probability maps accumulated over the edges\n",
    "# \"reg\" -> region statistics, mapped to the edges\n",
    "# \"topo\" -> topology features for the edges\n",
    "feat_list = (\"raw\", \"prob\", \"reg\", \"topo\")\n",
    "\n",
    "# we run multicuts for train -> train\n",
    "# and train -> test\n",
    "mc_nodes_train, mc_edges_train, mc_energy_train, t_inf_train = multicut_workflow(\n",
    "    ds_train, ds_train,\n",
    "    seg_id, seg_id,\n",
    "    feat_list, mc_params)\n",
    "mc_nodes_test, mc_edges_test, mc_energy_test, t_inf_test = multicut_workflow(\n",
    "    ds_train, ds_test\n",
    "    seg_id, seg_id,\n",
    "    feat_list, mc_params)\n",
    "\n",
    "# project the result back to the volume and merge small segments\n",
    "mc_seg_train = ds_train.project_mc_result(seg_id, mc_nodes_train)\n",
    "mc_seg_test = ds_test.project_mc_result(seg_id, mc_nodes_test)\n",
    "\n",
    "mc_seg_train = merge_small_segments(mc_seg_train, 100)\n",
    "mc_seg_test = merge_small_segments(mc_seg_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate the result\n",
    "print \"Train-RI:\", rand_index(mc_seg_train.ravel(), ds_train.gt().ravel())\n",
    "print \"Test-RI:\", rand_index(mc_seg_test.ravel(), ds_test.gt().ravel())\n",
    "\n",
    "# proper skneuro randindex\n",
    "\"\"\"\n",
    "from skneuro.learning import randIndex\n",
    "print randIndex(ds_train.gt().ravel(), mc_seg_train.ravel(), True)\n",
    "print randIndex(ds_test.gt().ravel(), mc_seg_test.ravel(), True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this numbers don't look good. But I think this is due to the sklearn metric...\n",
    "\n",
    "With the skneuro RandIndex, I obtain:\n",
    "\n",
    "RI-train: 0.9997\n",
    "\n",
    "RI-test: 0.9988"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}